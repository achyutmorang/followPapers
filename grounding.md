
# concept grounding / implicit learning

Oct.13, google "concept grounding   ICML NIPS"

## Papers
[CVPR 2018- Unsupervised Textual Grounding: Linking Words to Image Concepts](https://arxiv.org/pdf/1803.11185.pdf)

[CVPR 2018- Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in
Instructional Videos](http://vision.stanford.edu/pdf/huang-buch-2018cvpr.pdf)

[CVPR 2018- Finding beans in burgers:Deep semantic-visual embedding with localization](http://openaccess.thecvf.com/content_cvpr_2018/papers/Engilberge_Finding_Beans_in_CVPR_2018_paper.pdf)

[NIPS 2017- Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts](https://papers.nips.cc/paper/6787-interpretable-and-globally-optimal-prediction-for-textual-grounding-using-image-concepts.pdf)

[ICML 2018- Interpretability Beyond Feature Attribution:Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/pdf/1711.11279.pdf)

[ICML 2012 - A Joint Model of Language and Perception for Grounded Attribute Learning](https://homes.cs.washington.edu/~lsz/papers/mfzbf-icml12.pdf)

[2018 Multimodal Grounding for Language Processing](http://aclweb.org/anthology/C18-1197)

## NIPS Workshops
[Visually Grounded Interaction and Language 2017](https://nips2017vigil.github.io/)
[Visually Grounded Interaction and Language 2018](https://nips2018vigil.github.io/)

## ICML 2018 Notes
https://david-abel.github.io/blog/posts/misc/icml_2018.pdf